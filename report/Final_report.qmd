---
title: "Math 244: Final Report"
author: "Mia Zottoli, Vishnu Lakshman, Julia Vitale"
format: html
editor: visual
echo: false
warning: false
---

```{r load-packages}
#Load r packages here
library(tidyverse)
library(gtsummary)
library(ggplot2)
library(psych)
library(cluster)
library(factoextra)
library(dplyr)
library(reticulate)
library(here)
```

# Introduction and Data

The data set includes statistics from the 2024 Division 1 and 3 Men’s and Women’s Ultimate Frisbee Championships. The statistics were found on USA Ultimate, the non-profit organization serving as the governing body for ultimate in the United States, and were taken from a data visualization titled “USA Ultimate 2024 Nationals Stats Dashboard”, which was created by Ben Ayres. The data set includes 1665 rows which each correspond to an individual player, and it includes 15 variables which categorize the players by Division, Gender, and Team, and provide game statistics for each player. 

Ultimate Frisbee is a sport growing in popularity at the collegiate level and within the Vassar student body as well. However, not much data analysis is available for Ultimate compared to other popular sports. We want to fit a model that would help players analyze their game performance. Therefore, via linear, LASSO, and ridge regression we will fit a prediction model that we train under supervised conditions to be able to predict a player’s plus/minus score (AKA individual impact, calculated as the difference between the points scored and the points allowed while the player is on the court) based on the variables turns_per_game, ds_per_game and ast_per_game. These variables all relate to a player's effectiveness on the field, which is why we will use them to predict pls_mns_per_game. We will also stratify by division and gender, so that we ensure that we have an accurate model for each group. We will have four models in the final product, DI Men, DI Women, DIII Men, and DIII Women.

Since the data was already pretty clean, we did not have to do much data tidying. We just did some string manipulation to extract the school name from the team name, creating a new column called 'school', and had to convert some character variables to factors.

```{r load-data}
# Load datasets
ultimate_data <- read_csv('/Users/Vishnu/Documents/Classes/MATH 244 - Intermediate Data Science/M244-Final-JVM/data/ultimate_data.csv')
```

# Methodology

### *Summary Statistics*

```{r summary-statistics-for-relevant-variables}
# Print summary statistics
summary_data <- ultimate_data[, c("pls_mns_per_game", "turns_per_game", "ds_per_game", "ast_per_game", "pts_per_game")]

summary(summary_data)
```

```{r view-correlations-between-relevant-variables}
# Calculate correlations between variables
cor(summary_data, use = "complete.obs")
```

### *Visualizations/EDA*

```{r gender-&-plus-minus}
# Plot player's plus-minus scores and their gender
ggplot(ultimate_data, aes(x = gender, y = plus_minus, fill = division)) +
  geom_boxplot() +
  labs(
    title = "Gender vs. Player's +/- Score",
    x = "Division",
    y = "Player's +/- Score",
    fill = "Division") +
  theme_minimal()


```

```{r plus-minus-&-other-variable}

# Plot a player's plus-minus score and their points per game for the different divisions and genders
ultimate_data %>% ggplot(aes(x = pts_per_game, y = plus_minus, color = division)) + 
   geom_point() + geom_smooth(method = 'lm', se = F) + scale_x_log10() +
  labs(x = "Points per game", 
       y = "Plus/Minus", 
       color = "Division",
       title = "Points per game vs. player impact",
       subtitle = "By division and gender") + 
  theme_minimal() +
  scale_color_viridis_d()

# Plot a player's plus-minus score and their D's per game for the different divisions and genders
ultimate_data %>% ggplot(aes(x = ds_per_game, y = plus_minus, color = division)) + 
   geom_point() + geom_smooth(method = 'lm', se = F)+
  theme_minimal() + scale_x_log10() + 
  labs(x = "Ds per game", y = "Plus/Minus", color = "Division",
       title = "Ds per game vs. player impact",
       subtitle = "By division and gender") +
  scale_color_viridis_d()

# Plot a player's plus-minus score and their turns per game for the different divisions and genders
ultimate_data %>% ggplot(aes(x = turns_per_game, y = plus_minus, color = division)) + 
   geom_point() + geom_smooth(method = 'lm', se = F) + scale_x_log10() +
  labs(x = "Turns per game", y = "Plus/Minus", color = "Division", title = "Turns per game vs. player impact",
       subtitle = "By division and gender") + theme_minimal() +
  scale_color_viridis_d()

# Plot a player's plus-minus score and their assists per game for the different divisions and genders
ultimate_data %>% ggplot(aes(x = ast_per_game, y = plus_minus, color = division)) + 
   geom_point() + geom_smooth(method = 'lm', se = F) + scale_x_log10() +
  labs(x = "Assists per game", y = "Plus/Minus", color = "Division", title = "Assists per game vs. player impact",
       subtitle = "By division and gender") + theme_minimal() +
  scale_color_viridis_d()
```

## Modeling methodology

We are fitting all three models that predict continuous variables: linear, LASSO, and ridge regression. Based on the results, we will select the model that is most accurate. From the scatter plots for our four predictor variables against the outcome variable of +/-, we see a fairly linear relationship between the variables. For pts_per_game, all of the relationships are positively linear. For ds_per_game and ast_per_game, all of the relationships are positive except for Division 3 Women. Additionally, for turns_per_game, the regression lines for both Division 1 and 3 have negative slopes. These differences, along with the slight variations in the box plots for player's +/- score by Division and Gender, suggest that these predictors act differently by Division and Gender. Therefore, we will fit the model for each combination of division and gender, rather than utilizing a blanket model for all types of players.

# Results

```{python load-data-and-packages}
#Load python packages here

ultimate_data = r.ultimate_data

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.exceptions import DataConversionWarning
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_transformer
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GroupKFold
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.linear_model import LassoCV

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

### *Linear Regression*

```{python fit-lm-by-division-gender}
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def linear_reg_by_group(ultimate_data, test_size=0.2, random_state=42):
    feature_cols = ['turns_per_game', 'ds_per_game', 'ast_per_game']
    target_col = 'pls_mns_per_game'

    # Clean data
    data_clean = ultimate_data[feature_cols + [target_col] + ['level','gender']].dropna()

    # Initialize results list
    results = {}

 # Group by level and gender
    for (level, gender), group in data_clean.groupby(['level', 'gender']):
        X = group[feature_cols]
        y = group[target_col]

        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)

        # Fit model
        model = LinearRegression()
        model.fit(X_train, y_train)

        # Collect results
        score = model.score(X_test, y_test)  # R² score
        coef = dict(zip(feature_cols, model.coef_))
        intercept = model.intercept_

        # Save results in the dictionary with the (level, gender) tuple as the key
        results[(level, gender)] = {
            'score': score,
            'intercept': intercept,
            'coefficients': coef,
            'n_samples': len(group),
            'model': model,
            'scaler': scaler
        }

    return results

# Run the adapted function
results_lmdg = linear_reg_by_group(ultimate_data)

for (level, gender), result in results_lmdg.items():
    score = result['score']
    n_samples = result['n_samples']
    coefficients = result['coefficients']
    intercept = result['intercept']

    print(f"{level} {gender} — Mean R² (CV): {score:.3f} (n={n_samples})")
    print("  Coefficients:")
    for feat, coef in coefficients.items():
        print(f"    {feat}: {coef:.3f}")
    print(f"  Intercept: {intercept:.3f}")
    print()
```

### *Ridge Regression*

```{python fit-ridge-by-division-gender}
def ridge_reg_by_group2(ultimate_data, test_size=0.2, random_state=42, alpha=0.1):
    feature_cols = ['turns_per_game', 'ds_per_game', 'ast_per_game']
    target_col = 'pls_mns_per_game'

    # Clean data
    data_clean = ultimate_data[feature_cols + [target_col] + ['level','gender']].dropna()

    # Initialize results dictionary
    results = {}

    # Group by level and gender
    for (level, gender), group in data_clean.groupby(['level', 'gender']):
        X = group[feature_cols]
        y = group[target_col]

        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)

        # Fit model
        model = Ridge(alpha=alpha)
        model.fit(X_train, y_train)

        # Collect results
        score = model.score(X_test, y_test)  # R² score
        coef = dict(zip(feature_cols, model.coef_))
        intercept = model.intercept_

        # Save results in the dictionary with the (level, gender) tuple as the key
        results[(level, gender)] = {
            'score': score,
            'intercept': intercept,
            'coefficients': coef,
            'n_samples': len(group),
            'model': model,  # Save the trained model
            'scaler': scaler  # Save the scaler used for feature scaling
        }
        
    return results



# Run the adapted function
results_ridge2dg = ridge_reg_by_group2(ultimate_data)

for (level, gender), result in results_ridge2dg.items():
    score = result['score']
    n_samples = result['n_samples']
    coefficients = result['coefficients']
    intercept = result['intercept']

    print(f"{level} {gender} — Mean R² (CV): {score:.3f} (n={n_samples})")
    print("  Coefficients:")
    for feat, coef in coefficients.items():
        print(f"    {feat}: {coef:.3f}")
    print(f"  Intercept: {intercept:.3f}")
    print()

```

### *Lasso Regression*

## Function grouped by division and gender

```{python fit-lasso-by-division-gender}
import pandas as pd
from sklearn.linear_model import LassoCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Create function
def lasso_reg_by_group(ultimate_data, test_size=0.2, random_state=42):
    # Define features and target
    feature_cols = ['turns_per_game', 'ds_per_game', 'ast_per_game']
    target_col = 'pls_mns_per_game'

    # Drop rows with missing values
    data_clean = ultimate_data[feature_cols + [target_col] + ['level', 'gender']].dropna()

    # Initialize results dict
    results = {}

    # Group by level and gender
    for (level, gender), group in data_clean.groupby(['level', 'gender']):
        X = group[feature_cols]
        y = group[target_col]

        # Standardize features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)
        
        # Use LassoCV to find the best alpha
        model = LassoCV(alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10], cv=5)
        model.fit(X_train, y_train)
        
        # Make predictions
        y_pred = model.predict(X_test)

        # Output results
        score = model.score(X_test, y_test)  # R² score
        mae = mean_absolute_error(y_test, y_pred)
        rmse = mean_squared_error(y_test, y_pred) ** 0.5
        coef = dict(zip(feature_cols, model.coef_))
        intercept = model.intercept_
        best_alpha = model.alpha_

        results[(level, gender)] = {
            'score': score,
            'mae': mae,
            'rmse': rmse,
            'best_alpha': best_alpha,
            'coefficients': coef,
            'intercept': intercept,
            'n_samples': len(group),
            'model': model,
            'scaler': scaler
        }
    
    return results

# Run the adapted function
results_df_lasso = lasso_reg_by_group(ultimate_data)

for (level, gender), result in results_df_lasso.items():
    score = result['score']
    n_samples = result['n_samples']
    coefficients = result['coefficients']
    intercept = result['intercept']

    print(f"{level} {gender} — Mean R² (CV): {score:.3f} (n={n_samples})")
    print("  Coefficients:")
    for feat, coef in coefficients.items():
        print(f"    {feat}: {coef:.3f}")
    print(f"  Intercept: {intercept:.3f}")
    print()
```

```{python df-R^2-across-models}
# Create an empty list
rows = []

# Get all unique pairs from models dicts
all_keys = set(results_lmdg.keys()) | set(results_df_lasso.keys()) | set(results_ridge2dg.keys())

# Loop through each pair for R^2 
for key in all_keys:
    level, gender = key
    linear_r2 = results_lmdg.get(key, {}).get('score', None)
    lasso_r2 = results_df_lasso.get(key, {}).get('score', None)
    ridge_r2 = results_ridge2dg.get(key, {}).get('score', None)

    rows.append({
        'level': level,
        'gender': gender,
        'Linear Regression': linear_r2,
        'Lasso': lasso_r2,
        'Ridge': ridge_r2
    })

# Create df
r2_scores_df = pd.DataFrame(rows)

r2_scores_df = r2_scores_df.sort_values(by=['level', 'gender']).reset_index(drop=True)
```

```{r print-df}
# Print df on cross-validated R-squared values
temp <- py$r2_scores_df %>% mutate_if(is.numeric, round, 4)

library(gt)
temp %>% as_tibble() %>% gt() %>%
  tab_header(
    title = "Cross-validated R-sqaured values")
```

#### **Prediction Model**

We can use our model to predict player impact based on historical performance statistics. For example, we can predict the plus/minus score of a Division 1 Male athlete with an average of 2.5 turns per game, 1.0 Ds per game, and 4.0 assists per game.

```{python predict-plus-minus-score}

# Create function to predict plus-minus statistic
def predict_plus_minus_for_player(ultimate_data, level, gender, player_stats, results_ridge2dg, results_lmdg, results_df_lasso):
    
    feature_cols = ['turns_per_game', 'ds_per_game', 'ast_per_game']
  


    # Get pre-trained models and scalers
    ridge_model = results_ridge2dg[(level, gender)].get('model', None)
    ridge_scaler = results_ridge2dg[(level, gender)].get('scaler', None)
    
    linear_model = results_lmdg[(level, gender)].get('model', None)
    linear_scaler = results_lmdg[(level, gender)].get('scaler', None)
    
    lasso_model = results_df_lasso[(level, gender)].get('model', None)
    lasso_scaler = results_df_lasso[(level, gender)].get('scaler', None)

    # Check if all models and scalers are retrieved successfully
    if not ridge_model or not ridge_scaler:
        raise ValueError(f"Ridge model or scaler not found for {level} {gender}")
    if not linear_model or not linear_scaler:
        raise ValueError(f"Linear model or scaler not found for {level} {gender}")
    if not lasso_model or not lasso_scaler:
        raise ValueError(f"Lasso model or scaler not found for {level} {gender}")

    # Prepare the player stats as a DataFrame
    player_df = pd.DataFrame([player_stats], columns=feature_cols)

    # Scale the player stats using the pre-trained scalers
    ridge_scaled = ridge_scaler.transform(player_df)
    linear_scaled = linear_scaler.transform(player_df)
    lasso_scaled = lasso_scaler.transform(player_df)

    # Make predictions with the pre-trained models
    predicted_ridge = ridge_model.predict(ridge_scaled)[0]
    predicted_linear = linear_model.predict(linear_scaled)[0]
    predicted_lasso = lasso_model.predict(lasso_scaled)[0]

    return {
        'ridge_prediction': predicted_ridge,
        'linear_prediction': predicted_linear,
        'lasso_prediction': predicted_lasso
    }

# Example player stats
player_stats = {
    'turns_per_game': 2.5,
    'ds_per_game': 1.0,
    'ast_per_game': 4.0
}

# Example usage with pre-trained models (assuming you have the models already trained)
predictions = predict_plus_minus_for_player(ultimate_data, 'Division 1', 'Men', player_stats, results_ridge2dg, results_lmdg, results_df_lasso)

```

```{r print-model-predictions}
predictions <-  py$predictions
predictions %>% as.data.frame() %>% gt()
print("Predictions for Division 1 Male Player:")
paste("Ridge Prediction:", round(predictions$ridge_prediction, 3))
paste("Linear Prediction:", round(predictions$linear_prediction, 3))
paste0("Lasso Prediction: ", round(predictions$lasso_prediction, 3), "0")

```

All three models predict that this player has a plus/minus of approximately 4.6 (pretty good!). The models behave similarly, but have slight variations in prediction.

## Discussion and Conclusion

In this project, we explored the feasibility of building a predictive model to estimate a player's plus-minus score in college Ultimate Frisbee based on individual game statistics: turnovers per game, defensive blocks per game, and assists per game. We implemented three distinct regression models—Linear Regression, Ridge Regression, and Lasso Regression—to evaluate how well these features could predict the target outcome.

All three models (**Linear, Lasso and Ridge**) produced very similar predictions across multiple gender-division combinations, suggesting that the underlying relationships in the data are fairly linear and do not require strong regularization to prevent overfitting.

It should be mentioned that during the time of our preliminary analysis and presentation, our R² value was 1 in most cases, and close to 1 (e.g., 0.997) in the remaining cases, across all three models and categories. We later were able to identify that this was due to the inclusion of the explanatory variable `pts_per_game`, which we learnt was used directly in calculating the outcome variable. To address this, we excluded `pts_per_game` from the model and reran the analysis. After its removal, all subgroups across the models produced more realistic R² values, ranging from 0.7 to 0.8.

Additionally, in the preliminary analysis the **Lasso Regression** model was under performing. It was predicting plus-minus scores of 0.68 when the other two models, Linear and Ridge, were predicting 8.28 and 8.30 respectively. However, this was fixed through using cross-validation to find the best alpha (it was to high).

In summary, our results suggest that individual game statistics like turnovers, blocks, and assists can reasonably predict a player's plus-minus score using standard regression techniques. After refining the feature set and tuning model parameters through cross-validation, we achieved consistent and realistic performance across all models.

### Methodological Evaluation

To ensure consistent model evaluation, we split the dataset by both **division (D-I vs. D-III)** and **gender (men’s vs. women’s)**, and trained separate models for each subgroup. This decision was grounded in the understanding that the dynamics of ultimate frisbee can vary substantially across these categories, and that a model trained on pooled data might obscure those differences.

### Limitations

1.  **Small feature set**: Only three predictor variables were used. While these are intuitively related to performance, plus-minus is inherently a team-dependent metric influenced by many contextual factors (e.g., opponent strength, line composition, playing time) that are not captured here.
2.  **Potential data imbalance**: The performance of the models across different (division, gender) groups could vary depending on how many observations we had per group. If some subgroups had fewer examples, model stability would be affected.
3.  **No non-linear modeling**: While most of the relationships we observe appear somewhat linear in exploratory analysis, fitting a nonlinear model, such as a random forest, may be a better analysis route.

### Data Reliability and Validity

The dataset was sourced from a publicly available website and published within the past year. While not a first-party dataset, its reliability is bolstered by several key factors:

1.  It reflects official statistics from the most recent USA Ultimate College National Championships, meaning it's drawn from the highest college-level competition.
2.  As active Frisbee players ourselves, we were able to recognize nearly all the teams listed and even some individual players, affirming the data's authenticity and face validity.
3.  The dataset has had multiple downloads from the site, indicating community engagement and a degree of scrutiny.

### Appropriateness of the Analysis

Our analytical choices—using regularized regression methods and controlling for gender and division—are appropriate given our objective and dataset size. We standardized our features using `StandardScaler()` and implemented a consistent 80/20 train-test split across all models (as shown in the `lasso_reg` function), which helped avoid data leakage and ensured fair comparison between models.

Using multiple models allowed us to assess the sensitivity of plus-minus predictions to different regularization schemes. Ridge, in particular, proved effective in slightly dampening noisy relationships while preserving feature contributions.

### Ethical Considerations

Benefits:

-   Enhanced Performance Insights: Our model can provide players and coaches with data-driven insights, potentially informing training and strategy.

-   Objective Evaluation: By quantifying performance metrics, we aim to reduce subjective biases in player assessments.

Risks:

-   Privacy Concerns: Even though our data is publicly available, using it for predictive modeling raises questions about consent and the extent to which athletes are aware of or agree to such analyses.

-   Potential for Misuse: There's a risk that our model could be used to make decisions about player selection or playing time without considering the broader context of an athlete's performance or potential.

-   Bias and Fairness: If our model inadvertently reflects existing biases in the data, it could perpetuate inequalities, especially if used in decision-making processes.

Overall, this project demonstrates the potential of data-driven approaches to enhance our understanding of individual performance in Ultimate Frisbee. While our models show promise within the scope of elite college-level play, future work must address broader data inclusion to effective application.

## Given a second chance what we would have done differently

If we were to restart the project, we would have done a far more a rigorous analysis of the explanatory variables used. While all the variables currently chosen did show evidence affecting the target variable, we choose them based on our advanced knowledge and experience playing the sport and not through empirical means. It would strengthen the accuracy and reliability of our models results if could empirically confirm that these four are the most influential variables out of all the possible options in the dataset. To empirically test would include calculations such as making a correlation heatmap of all the explanatory variables against the outcome variable. Also, there could be a lot to gain if we modified our outcome variable so that it could be more informative. The plus-minus statistic is taken from basketball and while useful in Frisbee as well, it has its limitations. One of them is that in most Frisbee games there is an Offensive-line and Defensive-line with totally different players on each line. And the team on offensive has a far greater probability of scoring. Therefore, there could be some really good players on the Defensive-line who keep getting scored on and this statistic would unfairly represent them as worse than Offensive-line players even if that may not necessarily be the case.
