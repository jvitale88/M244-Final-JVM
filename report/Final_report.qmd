---
title: "Final_report"
format: html
editor: visual
echo: false
---

```{r load-packages}
library(tidyverse)
library(gtsummary)
library(ggplot2)
library(psych)
library(cluster)
library(factoextra)
library(dplyr)
library(reticulate)
library(here)
```

# Introduction and Data

The data set includes statistics from the 2024 Division 1 and 3 Men’s and Women’s Ultimate Frisbee Championships. The statistics were found on USA Ultimate, the non-profit organization serving as the governing body for ultimate in the United States, and were taken from a data visualization titled “USA Ultimate 2024 Nationals Stats Dashboard”, which was created by Ben Ayres. The data set includes 1665 rows which each correspond to an individual player, and it includes 15 variables which categorize the players by Division, Gender, and Team, and provide game statistics for each player. 

Ultimate frisbee is a sport growing in popularity at the collegiate level and within the Vassar student body as well. However, not much data analysis is available for Ultimate compared to other popular sports. We want to fit a model that would help players analyze their game performance. Therefore, via linear, LASSO, and ridge regression we will fit a prediction model that we train under supervised conditions to be able to predict a player’s plus/minus score (AKA individual impact, calculated as the difference between the points scored and the points allowed while the player is on the court) based on the variables turns_per_game, ds_per_game, ast_per_game, and pts_per_game. These variables all relate to a player's effectiveness on the field, which is why we will use them to predict pls_mns_per_game. We will also stratify by division and gender, so that we ensure that we have an accurate model for each group. We will have four models in the final product, DI Men, DI Women, DIII Men, and DIII Women.

Since the data was already pretty clean, we did not have to do much data tidying. We just did some string manipulation to extract the school name from the team name, creating a new column called 'school', and had to convert some character variables to factors.

```{r load-data}
here()
ultimate_data <- read_csv(here('data', "ulti_clean.csv"))
```

```{r}
```

# Methodology

### *Summary Statistics*

```{r summary-statistics-for-relevant-variables}
summary_data <- ultimate_data[, c("pls_mns_per_game", "turns_per_game", "ds_per_game", "ast_per_game", "pts_per_game")]

summary(summary_data)
```

```{r view-correlations-between-relevant-variables}
cor(summary_data, use = "complete.obs")
```

### *Visualizations/EDA*

```{r gender-&-plus-minus}
# Plot player's plus-minus scores and their gender
ggplot(ultimate_data, aes(x = gender, y = plus_minus, fill = gender)) +
  geom_boxplot() +
  labs(
    title = "Gender vs. Player's +/- Score",
    x = "Gender",
    y = "Player's +/- Score"
  ) +
  theme_minimal()

# Filter table to create a new table called d3 that only contains d3 level players
d3 <- ultimate_data[ultimate_data$level == "Division 3",]

# Plot  Division 3 level player's plus-minus scores and their gender
ggplot(d3, aes(x = gender, y = plus_minus, fill = gender)) +
  geom_boxplot() +
  labs(
    title = "Division 3 Gender vs. Player's +/- Score",
    x = "Gender",
    y = "Player's +/- Score"
  ) +
  theme_minimal()

# Filter table to create a new table called d1 that only contains d3 level players
d1 <- ultimate_data[ultimate_data$level == "Division 1",]

# Plot  Division 1 level player's plus-minus scores and their gender
ggplot(d1, aes(x = gender, y = plus_minus, fill = gender)) +
  geom_boxplot() +
  labs(
    title = "Division 1 Gender vs. Player's +/- Score",
    x = "Gender",
    y = "Player's +/- Score"
  ) +
  theme_minimal()
```

```{r plus-minus-&-other-variable}

# Plot a player's plus-minus score and their points per game for the different divisions and genders
ultimate_data %>% ggplot(aes(x = pts_per_game, y = plus_minus, color = division)) + 
   geom_point() + geom_smooth(method = 'lm', se = F) + scale_x_log10() +
  labs(x = "Points per game", y = "Plus/Minus", color = "Division") + 
  theme_minimal() +
  scale_color_viridis_d()

# Plot a player's plus-minus score and their D's per game for the different divisions and genders
ultimate_data %>% ggplot(aes(x = ds_per_game, y = plus_minus, color = division)) + 
   geom_point() + geom_smooth(method = 'lm', se = F)+
  theme_minimal() + scale_x_log10() + 
  labs(x = "Ds per game", y = "Plus/Minus", color = "Division") +
  scale_color_viridis_d()

# Plot a player's plus-minus score and their turns per game for the different divisions and genders
ultimate_data %>% ggplot(aes(x = turns_per_game, y = plus_minus, color = division)) + 
   geom_point() + geom_smooth(method = 'lm', se = F) + scale_x_log10() +
  labs(x = "Turns per game", y = "Plus/Minus", color = "Division") + theme_minimal() +
  scale_color_viridis_d()

# Plot a player's plus-minus score and their assists per game for the different divisions and genders
ultimate_data %>% ggplot(aes(x = ast_per_game, y = plus_minus, color = division)) + 
   geom_point() + geom_smooth(method = 'lm', se = F) + scale_x_log10() +
  labs(x = "Assists per game", y = "Plus/Minus", color = "Division") + theme_minimal() +
  scale_color_viridis_d()
```

We are fitting all three models that predict continuous variables: linear, LASSO, and ridge regression. Based on the results, we will select the model that is most accurate. From the scatter plots for our four predictor variables against the outcome variable of +/-, we see a fairly linear relationship between the variables. For pts_per_game, all of the relationships are positively linear. For ds_per_game and ast_per_game, all of the relationships are positive except for Division 3 Women. Additionally, for turns_per_game, the regression lines for both Division 1 and 3 have negative slopes. These differences, along with the slight variations in the box plots for player's +/- score by Division and Gender, suggest that these predictors act differently by Division and Gender. Therefore, we will fit the model for each combination of division and gender, rather than utilizing a blanket model for all types of players.

# Results

```{python load-data-and-packages}
ultimate_data = r.ultimate_data

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.exceptions import DataConversionWarning
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_transformer
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GroupKFold
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.linear_model import LassoCV

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

### *Linear Regression*

#### Function to Fit Model by Division and Gender

```{python fit-lm-by-division-gender}
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def linear_reg_by_group(ultimate_data, test_size=0.2, random_state=42):
    feature_cols = ['turns_per_game', 'ds_per_game', 'ast_per_game']
    target_col = 'pls_mns_per_game'

    # Clean data
    data_clean = ultimate_data[feature_cols + [target_col] + ['level','gender']].dropna()

    # Initialize results list
    results = {}

 # Group by level and gender
    for (level, gender), group in data_clean.groupby(['level', 'gender']):
        X = group[feature_cols]
        y = group[target_col]

        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)

        # Fit model
        model = LinearRegression()
        model.fit(X_train, y_train)

        # Collect results
        score = model.score(X_test, y_test)  # R² score
        coef = dict(zip(feature_cols, model.coef_))
        intercept = model.intercept_

        # Save results in the dictionary with the (level, gender) tuple as the key
        results[(level, gender)] = {
            'score': score,
            'intercept': intercept,
            'coefficients': coef,
            'n_samples': len(group),
            'model': model,
            'scaler': scaler
        }

    return results

# Run the adapted function
results_lmdg = linear_reg_by_group(ultimate_data)


```

### *Ridge Regression*

```{python try-dropping-pts_per_game}
def ridge_reg_by_group2(ultimate_data, test_size=0.2, random_state=42, alpha=0.1):
    feature_cols = ['turns_per_game', 'ds_per_game', 'ast_per_game']
    target_col = 'pls_mns_per_game'

    # Clean data
    data_clean = ultimate_data[feature_cols + [target_col] + ['level','gender']].dropna()

    # Initialize results dictionary
    results = {}

    # Group by level and gender
    for (level, gender), group in data_clean.groupby(['level', 'gender']):
        X = group[feature_cols]
        y = group[target_col]

        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)

        # Fit model
        model = Ridge(alpha=alpha)
        model.fit(X_train, y_train)

        # Collect results
        score = model.score(X_test, y_test)  # R² score
        coef = dict(zip(feature_cols, model.coef_))
        intercept = model.intercept_

        # Save results in the dictionary with the (level, gender) tuple as the key
        results[(level, gender)] = {
            'score': score,
            'intercept': intercept,
            'coefficients': coef,
            'n_samples': len(group),
            'model': model,  # Save the trained model
            'scaler': scaler  # Save the scaler used for feature scaling
        }
        
    return results



# Run the adapted function
results_ridge2dg = ridge_reg_by_group2(ultimate_data)

for (level, gender), result in results_ridge2dg.items():
    score = result['score']
    n_samples = result['n_samples']
    coefficients = result['coefficients']
    intercept = result['intercept']

    print(f"{level} {gender} — Mean R² (CV): {score:.3f} (n={n_samples})")
    print("  Coefficients:")
    for feat, coef in coefficients.items():
        print(f"    {feat}: {coef:.3f}")
    print(f"  Intercept: {intercept:.3f}")
    print()

```

### *Lasso Regression*


## Function grouped by division and gender

```{python}
import pandas as pd
from sklearn.linear_model import LassoCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

def lasso_reg_by_group(ultimate_data, test_size=0.2, random_state=42):
    # Define features and target
    feature_cols = ['turns_per_game', 'ds_per_game', 'ast_per_game']
    target_col = 'pls_mns_per_game'

    # Drop rows with missing values
    data_clean = ultimate_data[feature_cols + [target_col] + ['level', 'gender']].dropna()

    # Initialize results dict
    results = {}

    # Group by level and gender
    for (level, gender), group in data_clean.groupby(['level', 'gender']):
        X = group[feature_cols]
        y = group[target_col]

        # Standardize features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state)
        
        # Use LassoCV to find the best alpha
        model = LassoCV(alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10], cv=5)
        model.fit(X_train, y_train)
        
        # Make predictions
        y_pred = model.predict(X_test)

        # Output results
        score = model.score(X_test, y_test)  # R² score
        mae = mean_absolute_error(y_test, y_pred)
        rmse = mean_squared_error(y_test, y_pred) ** 0.5
        coef = dict(zip(feature_cols, model.coef_))
        intercept = model.intercept_
        best_alpha = model.alpha_

        results[(level, gender)] = {
            'score': score,
            'mae': mae,
            'rmse': rmse,
            'best_alpha': best_alpha,
            'coefficients': coef,
            'intercept': intercept,
            'n_samples': len(group),
            'model': model,
            'scaler': scaler
        }
    
    return results

# Run the adapted function
results_df_lasso = lasso_reg_by_group(ultimate_data)

```

#print results
```{python}
# Create an empty list to store each row of the DataFrame
rows = []

# Get all unique (level, gender) pairs from the model dictionaries
all_keys = set(results_lmdg.keys()) | set(results_df_lasso.keys()) | set(results_ridge2dg.keys())

# Loop through each key and collect R² values from each model
for key in all_keys:
    level, gender = key
    linear_r2 = results_lmdg.get(key, {}).get('score', None)
    lasso_r2 = results_df_lasso.get(key, {}).get('score', None)
    ridge_r2 = results_ridge2dg.get(key, {}).get('score', None)

    rows.append({
        'level': level,
        'gender': gender,
        'Linear Regression': linear_r2,
        'Lasso': lasso_r2,
        'Ridge': ridge_r2
    })

# Create the DataFrame
r2_scores_df = pd.DataFrame(rows)

# Optional: sort for readability
r2_scores_df = r2_scores_df.sort_values(by=['level', 'gender']).reset_index(drop=True)

# Display the result
```


```{r}
temp <- py$r2_scores_df %>% mutate_if(is.numeric, round, 4)

library(gt)
temp %>% as_tibble() %>% gt() %>%
  tab_header(
    title = "Cross-validated R-sqaured values")
```



#### **Prediction Model**

```{python}
def predict_plus_minus_for_player(ultimate_data, level, gender, player_stats, results_ridge2dg, results_lmdg, results_df_lasso):
    
    feature_cols = ['turns_per_game', 'ds_per_game', 'ast_per_game']
  


    # Get pre-trained models and scalers
    ridge_model = results_ridge2dg[(level, gender)].get('model', None)
    ridge_scaler = results_ridge2dg[(level, gender)].get('scaler', None)
    
    linear_model = results_lmdg[(level, gender)].get('model', None)
    linear_scaler = results_lmdg[(level, gender)].get('scaler', None)
    
    lasso_model = results_df_lasso[(level, gender)].get('model', None)
    lasso_scaler = results_df_lasso[(level, gender)].get('scaler', None)

    # Check if all models and scalers are retrieved successfully
    if not ridge_model or not ridge_scaler:
        raise ValueError(f"Ridge model or scaler not found for {level} {gender}")
    if not linear_model or not linear_scaler:
        raise ValueError(f"Linear model or scaler not found for {level} {gender}")
    if not lasso_model or not lasso_scaler:
        raise ValueError(f"Lasso model or scaler not found for {level} {gender}")

    # Prepare the player stats as a DataFrame
    player_df = pd.DataFrame([player_stats], columns=feature_cols)

    # Scale the player stats using the pre-trained scalers
    ridge_scaled = ridge_scaler.transform(player_df)
    linear_scaled = linear_scaler.transform(player_df)
    lasso_scaled = lasso_scaler.transform(player_df)

    # Make predictions with the pre-trained models
    predicted_ridge = ridge_model.predict(ridge_scaled)[0]
    predicted_linear = linear_model.predict(linear_scaled)[0]
    predicted_lasso = lasso_model.predict(lasso_scaled)[0]

    return {
        'ridge_prediction': predicted_ridge,
        'linear_prediction': predicted_linear,
        'lasso_prediction': predicted_lasso
    }

# Example player stats
player_stats = {
    'turns_per_game': 2.5,
    'ds_per_game': 1.0,
    'ast_per_game': 4.0
}

# Example usage with pre-trained models (assuming you have the models already trained)
predictions = predict_plus_minus_for_player(ultimate_data, 'Division 1', 'Men', player_stats, results_ridge2dg, results_lmdg, results_df_lasso)

# Print predictions
print(f"Predictions for Division 1 Male Player:")
print(f"  Ridge Prediction: {predictions['ridge_prediction']}")
print(f"  Linear Prediction: {predictions['linear_prediction']}")
print(f"  Lasso Prediction: {predictions['lasso_prediction']}")

```

Showcase how you arrived at answers to your research question using the techniques we have learned in class (and beyond, if you’re feeling adventurous).

**Provide only the main results from your analysis.** The goal is not to do an exhaustive data analysis (calculate every possible statistic and perform every possible procedure for all variables). Rather, you should demonstrate that you are proficient at asking meaningful questions and answering them using data, that you are skilled in interpreting and presenting results, and that you can accomplish these tasks using R and Python. More is not always better.

#### Discussion and Conclusion

In this project, we explored the feasibility of building a predictive model to estimate a player's plus-minus score in college Ultimate Frisbee based on individual game statistics: turnovers per game, defensive blocks per game, assists per game, and points per game. We implemented three distinct regression models—Linear Regression, Ridge Regression, and Lasso Regression—to evaluate how well these features could predict the target outcome.

Both **Linear** and **Ridge Regression** models produced very similar predictions across multiple gender-division combinations, suggesting that the underlying relationships in the data are fairly linear and do not require strong regularization to prevent overfitting.

In contrast, **Lasso Regression** consistently underperformed. For example, when provided with the input player_stats = { 'turns_per_game': 2.1, 'ds_per_game': 1.8, 'ast_per_game': 3.2, 'pts_per_game': 5.4 } its predicted plus-minus score was Ridge: 8.28, Linear: 8.30 and Lasso: 0.68. This significant deviation from the other two models suggests that Lasso may be overly aggressive in shrinking coefficients to zero—likely eliminating valuable predictors entirely. Given that we only have four features, the benefits of Lasso's feature selection are minimal, and its use may be unwarranted in this case.

### Methodological Evaluation

To ensure consistent model evaluation, we split the dataset by both **division (D-I vs. D-III)** and **gender (men’s vs. women’s)**, and trained separate models for each subgroup. This decision was grounded in the understanding that the dynamics of ultimate frisbee can vary substantially across these categories, and that a model trained on pooled data might obscure those differences.

## Limitations

1.  **Small feature set**: Only four predictor variables were used. While these are intuitively related to performance, plus-minus is inherently a team-dependent metric influenced by many contextual factors (e.g., opponent strength, line composition, playing time) that are not captured here.
2.  **Potential data imbalance**: The performance of the models across different (division, gender) groups could vary depending on how many observations we had per group. If some subgroups had fewer examples, model stability would be affected.
3.  **Lack of cross-validation metrics**: Although we evaluated the models qualitatively through predictions, we did not include quantitative metrics such as RMSE or R² for each subgroup. Including these would allow for a more robust evaluation of model accuracy and generalizability.

### Data Reliability and Validity

The dataset was sourced from a publicly available website and published within the past year. While not a first-party dataset, its reliability is bolstered by several key factors:

1.  It reflects official statistics from the most recent USA Ultimate College National Championships, meaning it's drawn from the highest college-level competition.
2.  As active Frisbee players ourselves, we were able to recognize nearly all the teams listed and even some individual players, affirming the data's authenticity and face validity.
3.  The dataset has had multiple downloads from the site, indicating community engagement and a degree of scrutiny.

### Appropriateness of the Analysis

Our analytical choices—using regularized regression methods and controlling for gender and division—are appropriate given our objective and dataset size. We standardized our features using `StandardScaler()` and implemented a consistent 80/20 train-test split across all models (as shown in the `lasso_reg` function), which helped avoid data leakage and ensured fair comparison between models.

Using multiple models allowed us to assess the sensitivity of plus-minus predictions to different regularization schemes. Ridge, in particular, proved effective in slightly dampening noisy relationships while preserving feature contributions. However, our choice of a fixed alpha parameter (rather than using cross-validation to tune hyperparameters) could limit model optimization and generalizability.

## Ethical Considerations

Benefits:

-   Enhanced Performance Insights: Our model can provide players and coaches with data-driven insights, potentially informing training and strategy.​

-   Objective Evaluation: By quantifying performance metrics, we aim to reduce subjective biases in player assessments.

Risks:

-   Privacy Concerns: Even though our data is publicly available, using it for predictive modeling raises questions about consent and the extent to which athletes are aware of or agree to such analyses.

-   Potential for Misuse: There's a risk that our model could be used to make decisions about player selection or playing time without considering the broader context of an athlete's performance or potential.

-   Bias and Fairness: If our model inadvertently reflects existing biases in the data, it could perpetuate inequalities, especially if used in decision-making processes.

Overall, this project demonstrates the potential of data-driven approaches to enhance our understanding of individual performance in ultimate Frisbee. While our models show promise within the scope of elite college-level play, future work must address broader data inclusion to effective application.

## What We Would Have Done Differently

If we were to restart the project, we would do far more a rigorous analysis of the variables used. While all the variables currently chosen did show evidence affecting the target variable, some additional work in order to confirm that these four are the most influencial out of all the possible options would have further.
